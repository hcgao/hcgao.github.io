<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>KDD 2023 Tutorial: Distributed Optimization for Big Data Analytics: Beyond Minimization</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-89918118-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="student.html">Students</a></div>
<div class="menu-item"><a href="services.html">Services</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>KDD 2023 Tutorial: Distributed Optimization for Big Data Analytics: Beyond Minimization</h1>
</div>
<ul>
<li><p>Date: Thursday, August 10</p>
</li>
<li><p>Time: 10:00am - 1:00 pm</p>
</li>
</ul>
<h2>Overview</h2>
<div class="infoblock">
<div class="blockcontent">
<p>The traditional machine learning model   can be formulated as an empirical risk minimization problem, which is typically  optimized via stochastic gradient descent (SGD).  
With the emergence of big data, distributed optimization, e.g., distributed SGD,  has been attracting increasing attention to facilitate machine learning models for big data analytics. 
However, existing distributed optimization mainly focuses on the standard minimization problem, failing to deal with the emerging machine learning models that are beyond that category. 
Of particular interest of this tutorial includes the <b>stochastic minimax optimization</b>, <b>stochastic bilevel optimization</b>, and <b>stochastic compositional optimization</b>, 
which covers a wide range of emerging machine learning models, e.g., model-agnostic meta-learning models, adversarially robust machine learning models, imbalanced data classification models, etc.  
Since these models have been widely used in big data analytics, it is necessary to provide a comprehensive introduction about the new distributed optimization algorithms designed for these models. 
Therefore, the goal of this tutorial is to present the state-of-the-art and  recent advances in distributed minimax optimization, distributed bilevel optimization, and distributed compositional optimization.  
In particular, we will introduce the typical applications in each category and discuss the corresponding distributed optimization algorithms in both centralized and decentralized settings.  
Through this tutorial, the researchers will be exposed to the fundamental algorithmic design and basic convergence theories, and the practitioners will be able to benefit from this tutorial to apply these algorithms to real-world applications.  </p>
</div></div>
<h2>Tutorial Outline</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>Section I:  Introduction (10:00am – 10:15am). <br /></p>
</li>
</ul>
<ul>
<li><p>Section II:  Distributed  Compositional Optimization (10:15am – 11:00am). <br /></p>
</li>
</ul>
<ul>
<li><p>Section III:  Distributed Minimax Optimization (11:05am – 11:50am). <br /></p>
</li>
</ul>
<ul>
<li><p>Break (10 mins). <br /></p>
</li>
</ul>
<ul>
<li><p>Section IV:  Distributed  Bilevel Optimization (12:00pm – 12:45pm).  <br /></p>
</li>
</ul>
<ul>
<li><p>Section V: Summary and Future Directions (12:50pm – 1:00pm). <br /></p>
</li>
</ul>
</div></div>
<h2>Presenters</h2>
<table class="imgtable"><tr><td>
<a href="IMGLINKTARGET"><img src="photo.JPG" alt="alt text" width="120px" height="140px" /></a>&nbsp;</td>
<td align="left"><p><b>Hongchang Gao</b> is an assistant professor in the Department of Computer and Information Sciences at Temple University. His research interests include machine learning, optimization, and biomedical data science, with a special focus on distributed optimization and federated learning. 
His works have been published at top venues, including ICML, NeurIPS, AISTATS, KDD, AAAI, IJCAI, etc. Currently, he serves as the Associate Editor of Journal of Combinatorial Optimization. He also has been serving multiple top machine learning and data mining conferences and journals as program committee members and reviewers.  
Recently, he was selected for  <b>AAAI 2023 New Faculty Highlights</b> program and received <b>Cisco Faculty Research Award</b>. </p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="IMGLINKTARGET"><img src="xinwen.jpg" alt="alt text" width="135px" height="140px" /></a>&nbsp;</td>
<td align="left"><p><b>Xinwen Zhang</b> is a Ph.D. student in the Department of Computer and Information Sciences at Temple University. 
Her research interest mainly focuses on stochastic minimax optimization and stochastic compostional optimization, as well as their applications on real-world data mining tasks. 
Before joining Temple University, she received her B.S. degree in Mathematics from Tongji University in China. 
Currently, she has completed several papers about distributed minimax optimization and distributed compositional optimization. </p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2025-06-09 21:15:37 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=EtsHne_TmseW5ch0xFRAaUP05lfRyCetP-g76VSS0gA"></script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("|");
    pageTracker._trackPageview();
} catch(err) {}</script>
</body>
</html>
